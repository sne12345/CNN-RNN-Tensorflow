{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 문장과 두 번째 문장의 자카트 유사도는 0.444444 입니다.\n",
      "첫 번째 문장과 세 번째 문장의 자카트 유사도는 0.000000 입니다.\n",
      "두 번째 문장과 세 번째 문장의 자카트 유사도는 0.083333 입니다.\n",
      "<전체 문장에 대한 BoW>\n",
      " {'나': 2, '는': 2, '어제': 2, '잠': 1, '을': 2, '못': 1, '잤습니다': 1, '밥': 3, '굶었습니다': 1, '오늘': 2, '은': 2, '맛': 1, '이': 1, '없습니다': 1, '.': 2, '별로': 1, '입니다': 1}\n",
      "첫 번째 문장의 벡터: [1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "두 번째 문장의 벡터: [1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "세 번째 문장의 벡터: [0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0.]\n",
      "첫 번째 문장과 두 번째 문장의 코사인 유사도는 0.617213 입니다.\n",
      "첫 번째 문장과 세 번째 문장의 코사인 유사도는 0.000000 입니다.\n",
      "[['나', '는', '어제', '잠', '을', '못', '잤습니다'], ['나', '는', '어제', '밥', '을', '굶었습니다'], ['오늘', '밥', '은', '맛', '이', '없습니다', '.'], ['오늘', '밥', '은', '별로', '입니다', '.']]\n",
      "[1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], [1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.], [0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0.]\n",
      "0.6172133998483676 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "\n",
    "# (다음 장에서 살펴볼) 원-핫 인코딩을 구현하는 함수입니다.\n",
    "def one_hot(x, depth):\n",
    "    output = []\n",
    "    for i in range(depth):\n",
    "        if i == x:\n",
    "            output.append(1)\n",
    "        else:\n",
    "            output.append(0)\n",
    "    return output\n",
    "\n",
    "\n",
    "def sum_list(X, depth):\n",
    "    output = np.zeros(depth)\n",
    "    for _list in X:\n",
    "        output = np.add(output, _list)\n",
    "    return output\n",
    "\n",
    "'''\n",
    "지시사항 1번\n",
    "BoW를 출력하는 함수를 만들어 봅니다.\n",
    "''' \n",
    "def bag_of_words(tokenized_sentences):\n",
    "    word_dict = {}\n",
    "    for tokenized_sentence in tokenized_sentences:\n",
    "        for token in tokenized_sentence:\n",
    "            word_dict[token] = word_dict.get(token,0)+1\n",
    "                \n",
    "            \n",
    "    return word_dict\n",
    "'''\n",
    "지시사항 2번\n",
    "자카드 유사도(Jaccard Similarity)를 구하는 함수를 만들어 봅니다.\n",
    "''' \n",
    "def jaccard(X, Y):\n",
    "    # TODO: 요소의 중복을 허용하는 리스트의 길이를 len_total에 저장합니다.\n",
    "\n",
    "    len_total = len(X+Y)\n",
    "    \n",
    "    \n",
    "    # TODO: 요소의 중복을 허용하지 않는 리스트(두 리스트의 합집합)의 길이를 len_union에 저장합니다.\n",
    "    len_union = len(set(X) | set(Y))\n",
    "    \n",
    "    \n",
    "    # TODO: len_total과 len_union을 이용하여 중복된 리스트(두 리스트의 교집합)의 길이를 len_inter 에 저장합니다.\n",
    "    len_inter = len_total - len_union\n",
    "    \n",
    "    return len_inter / len_union\n",
    "\n",
    "'''\n",
    "지시사항 3번\n",
    "Numpy를 이용하여(np.dot(), np.linalg.norm()) 코사인 유사도를 계산합니다.\n",
    "''' \n",
    "def cosine(x, y):\n",
    "    return np.dot(x,y) / ( np.linalg.norm(x) * np.linalg.norm(y) )\n",
    "\n",
    "def TextSimilarity():\n",
    "    sentences = [\"나는 어제 잠을 못 잤습니다\", \"나는 어제 밥을 굶었습니다\", \"오늘 밥은 맛이 없습니다.\", \"오늘 밥은 별로입니다.\"]\n",
    "    analyzer = Twitter()\n",
    "    tokenized = [ analyzer.morphs(sen) for sen in sentences ]\n",
    "\n",
    "    ## 토큰화된 문장을 바탕으로 자카드 유사도를 출력해 봅니다.\n",
    "    print('첫 번째 문장과 두 번째 문장의 자카트 유사도는 %f 입니다.' %(jaccard(tokenized[0], tokenized[1])))\n",
    "    print('첫 번째 문장과 세 번째 문장의 자카트 유사도는 %f 입니다.' % (jaccard(tokenized[0], tokenized[2])))\n",
    "    print('두 번째 문장과 세 번째 문장의 자카트 유사도는 %f 입니다.' % (jaccard(tokenized[1], tokenized[2])))\n",
    "    ## 토큰화된 문장을 바탕으로 BoW를 만듭니다.\n",
    "\n",
    "    BoW = bag_of_words(tokenized)\n",
    "    print(\"<전체 문장에 대한 BoW>\\n\",BoW)\n",
    "\n",
    "    ## {'word':index} 딕셔너리를 만듭니다.\n",
    "    word_index = { k:v for v,k in enumerate(BoW.keys())}\n",
    "\n",
    "    ## word_index_dict의 길이를 구하고 이를 바탕으로 원-핫 인코딩을 진행합니다.\n",
    "    len_wordIndex = len(word_index)\n",
    "\n",
    "    sen1 = [one_hot(word_index[token], len_wordIndex) for token in tokenized[0]]\n",
    "    sen2 = [one_hot(word_index[token], len_wordIndex) for token in tokenized[1]]\n",
    "    sen3 = [one_hot(word_index[token], len_wordIndex) for token in tokenized[2]]\n",
    "\n",
    "    ## 각각의 단어별 임베딩된 것을 더하여 문장의 임베딩 값으로 나타냅니다.\n",
    "    sen1_onehot = sum_list(sen1, len_wordIndex)\n",
    "    sen2_onehot = sum_list(sen2, len_wordIndex)\n",
    "    sen3_onehot = sum_list(sen3, len_wordIndex)\n",
    "\n",
    "    print(\"첫 번째 문장의 벡터:\",sen1_onehot)\n",
    "    print(\"두 번째 문장의 벡터:\", sen2_onehot)\n",
    "    print(\"세 번째 문장의 벡터:\", sen3_onehot)\n",
    "\n",
    "    ## 이를 바탕으로 코사인 유사도를 출력합니다.\n",
    "    sm_12 = cosine(sen1_onehot, sen2_onehot)\n",
    "    sm_13 = cosine(sen1_onehot, sen3_onehot)\n",
    "\n",
    "    print(\"첫 번째 문장과 두 번째 문장의 코사인 유사도는 %f 입니다.\" %sm_12)\n",
    "    print(\"첫 번째 문장과 세 번째 문장의 코사인 유사도는 %f 입니다.\" %sm_13)\n",
    "    print(tokenized)\n",
    "    print(str(sen1_onehot))\n",
    "    print(f\"{sen1_onehot}, {sen2_onehot}, {sen3_onehot}\")\n",
    "    print(sm_12, sm_13)\n",
    "    return tokenized, sen1_onehot, sen2_onehot, sen3_onehot, sm_12, sm_13\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    TextSimilarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
