{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 6, 7, 4, 1, 8, 1, 9, 10]\n",
      "tf.Tensor([0. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.], shape=(16,), dtype=float32)\n",
      "리스트 요소-인덱스 딕셔너리:  {'을': 1, '나': 2, '는': 3, '치킨': 4, '오늘': 5, '저녁': 6, '에': 7, '먹': 8, '예정': 9, '입니다': 10, '어제': 11, '맥주': 12, '와': 13, '함께': 14, '먹었': 15, '습니다': 16}\n",
      "\n",
      "정수값으로 변환된 sentence1: [2, 3, 5, 6, 7, 4, 1, 8, 1, 9, 10]\n",
      "\n",
      "정수값으로 변환된 sentence2: [2, 3, 11, 12, 13, 14, 4, 1, 15, 16]\n",
      "\n",
      "원-핫 인코딩된 문장1: [0. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      "\n",
      "원-핫 인코딩된 문장2: [0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "'''\n",
    "지시사항 1번\n",
    "embedding 함수를 완성합니다.\n",
    "\n",
    "   Step01. 입력된 리스트 'sentence1+sentence2'에 존재하는\n",
    "           요소마다 고유 인덱스를 붙입니다.\n",
    "           \n",
    "   Step02. 요소와 인덱스를 짝지은 딕셔너리 'word_dict'를 정의합니다.\n",
    "   \n",
    "   Step03. 'sentence1', 'sentence2'를 정수값으로 변환하고 이를\n",
    "           각각 리스트 변수 'sen1', 'sen2'로 정의합니다.\n",
    "'''\n",
    "\n",
    "def embedding(sentence1, sentence2):\n",
    "\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(sentence1+sentence2)\n",
    "    word_dict = tokenizer.word_index\n",
    "    \n",
    "    sen1 = tokenizer.texts_to_sequences(sentence1)\n",
    "    sen2 = tokenizer.texts_to_sequences(sentence2)\n",
    "    \n",
    "    \n",
    "    sen1 = [token[0] for token in sen1]\n",
    "    sen2 = [token[0] for token in sen2]\n",
    "    \n",
    "    return word_dict, sen1, sen2\n",
    "\n",
    "'''\n",
    "지시사항 2번\n",
    "텐서플로우를 사용하여 원-핫 인코딩을 실행합니다.\n",
    "'''   \n",
    "\n",
    "def one_hot(sen1, sen2, word_dict):\n",
    "\n",
    "    print(sen1)\n",
    "    \n",
    "    \n",
    "    oh_sen1 = sum(tf.one_hot(sen1, len(word_dict)))\n",
    "    oh_sen2 = sum(tf.one_hot(sen2, len(word_dict)))\n",
    "    \n",
    "    print(oh_sen1)\n",
    "    \n",
    "    \n",
    "    return oh_sen1, oh_sen2\n",
    "\n",
    "def main():\n",
    "    \n",
    "    sentence1 = ['나','는','오늘','저녁','에','치킨','을','먹','을','예정','입니다']\n",
    "    sentence2 = ['나','는','어제', '맥주','와', '함께', '치킨','을', '먹었', '습니다']\n",
    "    \n",
    "    word_dict, seq_1, seq_2 = embedding(sentence1, sentence2)\n",
    "    onehot_sen1, onehot_sen2 = one_hot(seq_1, seq_2, word_dict)\n",
    "        \n",
    "    print('리스트 요소-인덱스 딕셔너리: ', word_dict)\n",
    "    \n",
    "    print('\\n정수값으로 변환된 sentence1:', seq_1)\n",
    "    print('\\n정수값으로 변환된 sentence2:', seq_2)\n",
    "    \n",
    "    print('\\n원-핫 인코딩된 문장1:', onehot_sen1.numpy())\n",
    "    print('\\n원-핫 인코딩된 문장2:', onehot_sen2.numpy())\n",
    "    \n",
    "    return onehot_sen1, onehot_sen2\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
